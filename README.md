# Task-1(Data Cleaning and Data Pre-Processing)
This Python code demonstrates a complete data cleaning and preprocessing pipeline for machine learning using pandas, NumPy, matplotlib, seaborn, and scikit-learn. It begins by loading a raw CSV dataset and displaying its structure using .head(), .info(), and .describe() to understand column types, missing values, and statistical summaries. A heatmap is then used to visualize missing data, which is handled by either dropping or filling values. The code removes duplicates, converts columns to appropriate types (like datetime), and encodes categorical data using label encoding and one-hot encoding. It visualizes numerical feature distributions and detects outliers using histograms and boxplots, then removes outliers with the Z-score method. All numerical columns are scaled using StandardScaler to standardize feature values. Finally, the dataset is split into training and testing sets using an 80/20 ratio, preparing the data for training machine learning models. This pipeline ensures clean, consistent, and model-ready data.More actions
This code loads the Titanic dataset from Kaggle using pandas and begins by exploring its structure with .head(), .info(), and .describe(). A heatmap is plotted using seaborn to visualize missing data. The 'Cabin' column is dropped due to excessive missing values, while missing values in 'Age' are filled with the median and 'Embarked' with the mode. Categorical columns like 'Sex' and 'Embarked' are label-encoded or one-hot encoded appropriately. Unnecessary columns such as 'Name', 'Ticket', and 'PassengerId' are dropped to avoid irrelevant noise. Histograms and boxplots are used to visualize distributions and identify outliers, which are then removed using Z-score filtering from the scipy.stats module. Numerical features are scaled using StandardScaler, and finally, the cleaned dataset is split into training and testing sets using train_test_split, preparing it for machine learning model training with a focus on the target column 'Survived'.
